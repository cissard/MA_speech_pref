---
title: "How does the human auditory system become expert in speech processing? Insights from development."
author: "Cécile Issard and Alejandrina Cristia"
date: "`r format(Sys.time(), '%d %m, %Y')`"
output: pdf_document
#bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE) #option for all the code chunks of the document.
ggplot2::theme_set(langcog::theme_mikabr(base_family = "Ubuntu"))

source("compute_es.R", chdir = TRUE)  #chdir stands for "change directory"

library(metafor)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(purrr)
library(langcog)
library(grid)
library(gridExtra)
library(RCurl)

sessionInfo()
```
- Target journal: Developmental science
- Article type: short report
- 4000 words
- 6 keywords
- Running title: 40 characters
- Submit one normal and one blinded version
- Separate files for title page, main text, and figures
- No identifiying info in the main text.
- up to 4 research highlights; each 25 words
- Abstract: 250 words

Main text file:

1. Title 
2. Research highlights
3. Abstract and key words
4. Main 
5. References
6. Figures and tables (each clearly identified, labelled and on a separate page)
7. Appendices (if relevant).

```{r db general}

# Comment out next set of lines for RECALCULATION
 require(RCurl)
 u <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vRzzqtgNdfoKTMqb4bWyy5LyH5XdrOEy4sl3VNDCnGIyvdrny4wwUBeKPvy8tXczN0ri0yp94Kxgun_/pub?gid=0&single=true&output=csv"
 tc <- getURL(u, ssl.verifypeer=FALSE)
 DB <- read.csv(textConnection(tc))
 write.csv(DB,"MA_speech_pref_data.csv")
# Uncomment next line for OFFLINE MODE
#DB <- read.csv("MA_speech_pref_data.csv", header = T, sep = ",", na.strings = "")

## of datapoints and variables coded
dim(DB)

## FIX, remove empty columns
rmcol=NULL
for(mycol in 1:dim(DB)[2]) if(sum(is.na(DB[,mycol]))==length(DB[,mycol])) rmcol=c(rmcol,colnames(DB)[mycol])
rmcol[!(rmcol %in% "corr")]->rmcol
DB[,!(colnames(DB) %in% rmcol)]->DB
dim(DB)

## FIX, DOUBLE CHECK ALEX & CECILE !! REPLACE ALL EMPTY WITH NA
for(mycol in colnames(DB)) DB[DB[,mycol]=="" & !is.na(DB[,mycol]),mycol]<-NA
for(mycol in c("naturalness","social2","species2","test_lang")) DB[,mycol]<-factor(DB[,mycol]) 
summary(DB)

#unique studies
papers <- levels(factor(DB$short_cite))
paste('A total of', length(papers), 'papers were included.')

#number of unique infants
DB$n<-rowSums( cbind (DB$n_1,DB$n_2), na.rm=TRUE)
temp<-aggregate(n~same_infant,DB,mean)
paste('A total of', round(sum(temp$n)), 'infants contributed to the analysis')

summary(DB$mean_age_1)

```

# Introduction
A long line of research shows that infants process speech preferentially over other types of sounds. As the main signal for vocal communication, speech must be special for humans. Readily from birth, humans would be equipped with an auditory module dedicated to speech sounds, to process them with dedicated auditory and cognitive mechanisms. This preference has been investigated by numerous studies, contrasting speech to a variety of sounds, from white noise to backward speech, and at different ages. Getting a precise overview of this capacity is therefore difficult. 
“broader template that initially encompasses vocalizations of human and nonhuman primates and is rapidly tuned specifically to human vocalizations.” “Is this link sufficiently broad to include naturalistic vocalizations beyond those of our closest genealogical cousins, or is it restricted to primates, whose vocalizations may be perceptually just close enough to our own to serve as early candidates for the platform on which human language is launched?” (Ferry et al., 2013) 
The auditory literature suggests that natural sounds are processed differently by the auditory system (e.g. Mezhrahi & Nelken, 2014). Extending to language acquisition, naturalness is a key factor for word segmentation (Black and Bergman, 2016). Speech might therefore not be prefered per se, but because it belongs to a broader category of natural, own-species, or communicative sounds.

	To answer this question, we conducted a meta-analysis investigating infants’ preference for speech sounds over other types of sounds. 


1. Para/subsection 1: general para about preferences being useful for info filtering in all species; 
previous work shows these are partially innate, partially based on experience (in chicks: what can be used for imprinting or not; in humans: faces, voices, etc); 
End saying “here we synthesize empirical data on infants’ preferences for natural sounds, human sounds, social sounds, and/or speech”

2. Para/subsection 2: (age)
elaborate on developmental & experiential aspects (explain how they are correlated, and when they are separable)
→  age is a crucial factor; 
mention controversies in additional para if they exist

3. Para/subsection 3: (types)
types of attractors: natural sounds, human sounds, social sounds, and/or speech; 
explain that to a certain extent these are inside a hierarchy (show in a figure?); Nat0ural > human > social > vocal > speech Vent diagram.
also explain similarities and differences; 
explain the relevance of the contrast/control sound; 
if there are controversies, mention them as well (break down into several para if needed)

4. Para/subsection 4: MA
infant studies are difficult, so few of them and each with few participants
trying to address Qs like dev changes & differences across types would require large power
MA is a way of achieving power without running new studies

- a disadvantage is mixing together so less control & thus can miss subtle effects
- but several advantages: 

  -   reproducibility checks are built in (i.e., if we do find an effect, it’s more likely to be an effect different labs can find)
  -   can detect bias

# Methods
## Literature search
We followed PRISMA (Moher et al., 2009).

The information sources used to compose the initial list included suggestions by experts (authors of this work); two google scholar searches (“ ("speech preference" OR "own-species vocalization" ) AND infant”, and “("speech preference" OR "own-species vocalization" ) AND infant”) complemented with the same searches in PubMed and PsycInfo; and a google alert, as well as reference lists of the full papers inspected.

We included studies that tested human infants for birth to 1 year (0-365 days) of age, and contrasted speech sounds with any other type of sound, measuring either behavioral (e.g. looking times) or neurophysiological responses to the sounds. We excluded studies that contrasted foreign to native language, or didn’t present natural speech sounds. A PRISMA flowchart summarizes the literature review and selection process (Figure 1). We documented all the studies that we inspected in this decision spreadsheet.

[Insert Figure 1 here]

Data were coded by the first author. 20% of the papers were selected to be coded by the second author independently, with disagreements resolved by discussion. There were **XX** disagreements out of a total of **YY** fields filled in, so that the total agreement rate was **ZZ**%. The full list of the variables coded is available in the supplementary material.

**Risk assessment at the level of papers was done by ... Risk assessment for the whole body of literature ...**

## Statistical analysis

### Individual effect sizes

Once the data were coded, we computed individual effect sizes that were not directly reported in the papers, along with their respective variance. We adjusted the formula according to the experimental design of the respective paper (Lipsey \& Wilson, 2001). When the coded study used a within-participant design with two measurements (e.g. looking time during speech and during monkey calls), we computed effect size using t-statistic (Dunlap et al., 1996). If this statistic was not reported, we computed effect size based on the respective means and SDs.
We then corrected the computed effect size with the correlation between the two measurments. We computed this correlation based on the t-statistic, the respective means and SDs (Lipsey \& Wilson, 2001). If not all of these informations were reported, we randomly imputed a correlation with equal probability between 0.01 and 0.99. 

Effect sizes were first computed as Cohen's d, and then transformed to Hedge's g. 

```{r data_completion, include=FALSE}
#calculate correlations
for (i in 1:nrow(DB)){
    db = DB[i,]
       if (db$participant_design == "within_two") {
        # Use raw means, SD, and t-values to calculate correlations
          if (is.na(db$corr) & complete(db$x_1, db$x_2, db$SD_1, db$SD_2, db$t)) {
        db$corr = (db$SD_1^2 + db$SD_2^2 - (db$n_1 * (db$x_1 - db$x_2)^2 / db$t^2)) / (2 * db$SD_1 * db$SD_2)
          }
        DB[i,] = db
       }
}

#if all of these measures are not reported, use an imputed correlation value
#we also account for the observation that some re-calculated values are impossible and replace those
for (i in 1:nrow(DB)){
    db = DB[i,]
       if (db$participant_design == "within_two") {
         if (is.na(db$corr) | db$corr > .99 | db$corr < .01){
          db$corr = runif(1, min = 0.01, max = 0.99)
         }
       DB[i,] = db
       }
}

#We create variables for effect sizes (ES)
DB$d_calc = NA
DB$d_var_calc = NA
DB$g_calc = NA
DB$g_var_calc = NA
DB$r_calc = NA
DB$r_var_calc = NA
DB$z_calc = NA
DB$z_var_calc = NA
DB$log_odds_calc = NA
DB$log_odds_var_calc = NA
DB$es_method = NA
  
#we introduce variables d_calc and d_var_calc to distiguish them from the fields d and d_var, which are fields where effect sizes were already available from the source of the data
d_calc <- NA
d_var_calc <- NA
es_method <- "missing"
  
#start of decision tree where effect sizes are calculated differently based on participant design depending on which data is available, effect sizes are calculated differently
for (i in 1:nrow(DB)){
    db = DB[i,]
       if (db$participant_design == "between") {
    es_method  <- "between"
    #effect size calculation
    if (complete(db$x_1, db$x_2, db$SD_1, db$SD_2)) {
      pooled_SD <- sqrt(((db$n_1 - 1) * db$SD_1 ^ 2 + (db$n_2 - 1) * db$SD_2 ^ 2) / (db$n_1 + db$n_2 - 2)) # Lipsey & Wilson, 3.14
      d_calc <- (db$x_1 - db$x_2) / pooled_SD # Lipsey & Wilson (2001)
    } else if (complete(db$t)) {
      d_calc <- db$t * sqrt((db$n_1 + db$n_2) / (db$n_1 * db$n_2)) # Lipsey & Wilson, (2001)
    } else if (complete(db$F)) {
      d_calc <- sqrt(db$F * (db$n_1 + db$n_2) / (db$n_1 * db$n_2)) # Lipsey & Wilson, (2001)
    } else {d_calc = NA}
    #now that effect size are calculated, effect size variance is calculated
    if (complete(db$n_1, db$n_2, d_calc)) {
      d_var_calc <- ((db$n_1 + db$n_2) / (db$n_1 * db$n_2)) + (d_calc ^ 2 / (2 * (db$n_1 + db$n_2)))
    } else if (complete(db$d, db$d_var)) {
      #if d and d_var were already reported, use those values
      d_calc <- d
      d_var_calc <- d_var
    } else {d_var_calc = NA}

  } else if (db$participant_design == "within_two") {
      
    #effect size calculation
    if (complete(db$x_1, db$x_2, db$SD_1, db$SD_2)) {
      pooled_SD <- sqrt((db$SD_1 ^ 2 + db$SD_2 ^ 2) / 2) # Lipsey & Wilson (2001)
      d_calc <- (db$x_1 - db$x_2) / pooled_SD # Lipsey & Wilson (2001)
      es_method  <- "group_means_two"
    } else if (complete(db$t)) {
      wc <- sqrt(2 * (1 - db$corr))
      d_calc <- (db$t / sqrt(db$n_1)) * wc #Dunlap et al., 1996, p.171
      es_method  <- "t_two"
    } else if (complete(db$F)) {
      wc <- sqrt(2 * (1 - db$corr))
      d_calc <- sqrt(db$F / db$n_1) * wc
      es_method  <- "f_two"
    } else {d_calc = NA}
    #now that effect size are calculated, effect size variance is calculated
    if (complete(db$n_1, d_calc)) {
      #d_var_calc <- ((1 / n_1) + (d_calc ^ 2 / (2 * n_1))) * 2 * (1 - corr) #we used this until 4/7/17
      d_var_calc <- (2 * (1 - db$corr)/ db$n_1) + (d_calc ^ 2 / (2 * db$n_1)) # Lipsey & Wilson (2001)
    } else if (complete(db$d, db$d_var)) {
      #if d and d_var were already reported, use those values
      d_calc <- db$d
      d_var_calc <- db$d_var
      es_method  <- "d_two"
    } else {d_var_calc = NA}
    
  } else if (db$participant_design == "within_one") {
    if (complete(db$x_1, db$x_2, db$SD_1)) {
      d_calc <- (db$x_1 - db$x_2) / db$SD_1
      es_method  <- "group_means_one"
    } else if (complete(db$t)) {
      d_calc <- db$t / sqrt(db$n_1)
      es_method  <- "t_one"
    } else if (complete(db$F)) {
      d_calc <- sqrt(db$F / db$n_1)
      es_method  <- "f_one"
    } else {d_calc = NA}
  }
  
  df <- if (db$participant_design == "between") {
    sum(db$n_1, db$n_2, na.rm = TRUE) - 2
  } else {
    db$n_1 - 1
  }
  J <- 1 - 3 / (4 * (df - 1))
  g_calc <- d_calc * J
  g_var_calc <- J ^ 2 * d_var_calc

  if (db$participant_design == "between") {
    a <- (sum(db$n_1, db$n_2, na.rm = TRUE) ^ 2) / prod(db$n_1, db$n_2, na.rm = TRUE)
  } else {
    a <- 4
  }
  r_calc <- d_calc / sqrt(d_calc ^ 2 + a)
  r_var_calc <- a ^ 2 * d_var_calc / (d_calc ^ 2 + a) ^ 3

  z_calc <- 0.5 * log((1 + r_calc) / (1 - r_calc))
  z_var_calc = 1 / (db$n_1 - 3)

  log_odds_calc <- d_calc * pi / sqrt(3)
  log_odds_var_calc <- d_var_calc * pi ^ 2 / 3
  
  #add the results to the database
  db$d_calc = d_calc
  db$d_var_calc = d_var_calc
  db$g_calc = g_calc
  db$g_var_calc = g_var_calc
  db$r_calc = r_calc
  db$r_var_calc = r_var_calc
  db$z_calc = z_calc
  db$z_var_calc = z_var_calc
  db$log_odds_calc = log_odds_calc
  db$log_odds_var_calc = log_odds_var_calc
  db$es_method = es_method
  
  DB[i,] = db
}

#Mark effect sizes more than 3 SD away from the mean effect (in both positive and negative directions) as outliers
DB$outlier <- F #create the variable, and set as no by default (majority of cases hopefully!)
for (i in 1:nrow(DB)){
   db = DB[i,]
   if (!is.na(db$d_calc)){ #select the lines for which there is an ES available (bug next line if d_calc = NA)
     if (db$d_calc > mean(DB$d_calc, na.rm = TRUE) + 3*sd(DB$d_calc, na.rm = TRUE) | db$d_calc < mean(DB$d_calc, na.rm = TRUE) - 3*sd(DB$d_calc, na.rm = TRUE)) {
     db$outlier=T #if more than 3 SDs away from the mean, we consider this ES as an outlier.
     }
   }
   DB[i,] = db
}
#Visualize the outliers
outliers<-subset(DB,outlier==T)
outliers

#Not considered as outliers but still high
highES<-subset(DB,g_calc>(mean(DB$g_calc,na.rm=T)+2*sd(DB$g_calc,na.rm=T)))
highES

#centering of age (although some rows also have mean_age_2, it is always the same as mean_age_1 in this db, hence the latter is used)
DB$agec<-scale(DB$mean_age_1,scale=F)

# save the complete data base
write.csv(DB,'speech_pref_full_DB.csv')

summary(DB)

#info 
paste("We were considering", nrow(DB),"ES candidates")
paste("We could calculate", sum(!is.na(DB$d_calc)),"ES's")
table(DB$response_mode)
#summarize the data
mean(DB$g_calc,na.rm=T)
sd(DB$g_calc,na.rm=T)
stripchart(DB$g_calc, main = 'Distribution of ES')
boxplot(DB$g_calc)
```

### Meta-analytic models

Once the data was completed, we estimated the true effect size fitting mixed-effects meta-analytic regressions. We used the R package metafor **(CITE)**. We included random effects of paper, and random effects for independent infants within paper (same_infant).
We included the following variables in the analysis:
- Number and demographic characteristics of children, including age and gender;
- The experimental method (Central fixation/Head-turn Preference Procedure/High Amplitude Sucking/Passive Listening)
- Familiarity with the language used (native/foreign);
- Naturalness of the contrastive sound (natural/artificial);
If the sound contrasted to speech was natural, we also coded whether it was vocal or not, and from human or another species (homospecific/heterospecific).

We first assessed whether the experimental method influenced the magnitude of the effect size apart from target moderators by fitting a meta-analytic regression with the method as a moderator.

We investigated the effect of familiarity with the sound by running a meta-analytic model with nativeness of the language used for the speech stimuli as a moderator.

We then investigated whether speech preference was embedded in a preference for natural sounds, and whether this potential effect evolved over the course of the first year of life, by fitting a meta-analytic model with naturalness and age as moderators. To facilitate result interpretaion, we centered age.

Try a single model with age*test_lang*naturalness

Sho: We assess significance of predictor variables by model comparison.

Finally, we subsetted the dataset to contrasts between speech and natural sounds, and fitted a meta-analytic regression on this subset with socialness (social/non-social), vocalness (vocal/non-vocal), and species (homospecific/heterospecific) as moderators.

```{r moderators}
#distribution of moderators
g_age <- tapply(DB$g_calc,DB$mean_age_1,mean)#ES by age
table(DB$naturalness)#no. of natural and non-natural data points
table(DB$social)#no. of social and non-social data points
table(DB$species)#no. of homo- and heterospecific data points
table(DB$species,DB$social)
table(DB$naturalness,DB$social)

#plot ES as a function of moderators
#age
plot(unique(DB$mean_age_1)[1:33],g_age, main = 'ES as a function of age', xlab = 'age centered (days)', ylab = 'g_calc')
#naturalness
boxplot(DB$g_calc~DB$naturalness)

#social
boxplot(DB$g_calc~DB$social)

#species
boxplot(DB$g_calc~DB$species)
```


## Including Plots

```{r Figure Template}

apatheme=theme_bw()+
  theme(#panel.grid.major=element_blank(),
        #panel.grid.minor=element_blank(),
        panel.border=element_blank(),
        axis.line=element_line(),
        #text=element_text(family='Times'),
        legend.position='none')
```


```{r models}
#setting up of predictors 
#http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/

 #check that the experimental method doesn't make a difference (i.e. eye-tracking vs NIRS vs behavior)
#dummy coding: each level is compared to a reference level of the dependent variable, intercept corresponds to the reference level.
contrasts(DB$response_mode) = contr.treatment(length(levels(DB$response_mode)))
 #check that the language doesn't make a difference (true speech preference and not pref for native language, hence familiarity)
contrasts(DB$test_lang) <- contr.treatment(length(levels(DB$test_lang)))

 #moderators of interest
contrasts(DB$naturalness) <- contr.treatment(length(levels(DB$naturalness)),base=2)

DB$species<-factor(DB$species)
contrasts(DB$species) <- contr.treatment(length(levels(DB$species)))

DB$social<-factor(DB$social)
contrasts(DB$social) <- contr.treatment(length(levels(DB$social)),base=2)

#fit models
 #check that the method used doesn't make a difference
plot(DB$g_calc~DB$response_mode)
tapply(DB$g_calc,DB$response_mode, mean,na.rm=T)
tapply(DB$g_calc,DB$response_mode, sd,na.rm=T)
base_model <-rma.mv(g_calc, g_var_calc, mods = ~method, random = ~ 1 | study_ID/same_infant, data=DB, weighted=TRUE, method = "ML", subset=!DB$outlier & !is.na(DB$response_mode)) # We use method="ML", which is appropriate for model comparison

 #age varies with method (nirs<behavior<eye tracking)
tapply(DB$mean_age_1,DB$response_mode,mean,na.rm=T)
#Maybe response_mode is not the right variable for this.
table(DB$method,DB$response_mode)
#plot g as a function of method
mycols=c("black","blue","red","green")
names(mycols)<-levels(DB$method)
mycols
plot(DB$g_calc~DB$mean_age_1,col=mycols[DB$method],pch=20,cex=.7)

 #check also for test_lang (nativeness)
 #The language used for the speech trials doesn't make a difference
plot(DB$g_calc~DB$mean_age_1,col=DB$test_lang)
tapply(DB$g_calc,DB$test_lang, mean,na.rm=T)
tapply(DB$g_calc,DB$test_lang, sd,na.rm=T)

base_model2 <-rma.mv(g_calc, g_var_calc, mods = ~ test_lang, random = ~ 1 | study_ID/same_infant, data=DB, weighted=TRUE, method = "ML", subset=!DB$outlier & !is.na(DB$g_calc))

 #full model (with moderators of interest). As species and social are not orthogonal to naturalness, we only test for naturalness.
full_model <-rma.mv(g_calc, g_var_calc, mods= ~ naturalness, random = ~ 1 | study_ID/same_infant, data=DB, weighted=TRUE, method = "ML", subset=!DB$outlier,slab =DB$short_cite)

summary(full_model)

#subset to natural sounds, as naturalness doesn't make a difference. This allows to test species and social.
natural_only <-rma.mv(g_calc, g_var_calc, mods=~ species2*social2*agec, random = ~ 1 | study_ID/same_infant, data=DB, weighted=TRUE, method = "ML", subset=!DB$outlier & !is.na(DB$naturalness) & DB$naturalness=='natural')


summary(natural_only)
```

```{r plots}
forest.rma(full_model, main = 'forest plot of effect sizes', xlab = 'Hedges\' g') 

```

### Publication bias

We assessed the presence of a potantial publication bias in the literature by plotting funnel plot. We tested the asymmetry of the funnel plot by regressing effect size as a function of effect size standard error and running a Kendall's tau rank test. 

```{r publication bias}
tf <- trimfill(full_model)
fun.fig <- funnel(tf, cex=1.5, xlab='Hedges\' g', ylab="Standard Error of Effect Size g", digits=2, main="Funnel plot speech preference")

#Figure
pdf("Fig1.pdf")
par(mfrow=c(1,2)) #graphical parameters: A vector of the form c(nr, nc). Subsequent figures will be drawn in an nr-by-nc array on the device by rows (mfrow), respectively.

funnel(base_model,cex=1.5,xlab='Hedge\'s g', ylab="Standard Error of Effect Size g", digits=2, main="Funnel plot speech preference")
dev.off()
#add symmetrize

# testing for asymetry (indicates a publication bias)
regtest(DB$g_calc,DB$g_var_calc)
ranktest(DB$g_calc,DB$g_var_calc)

#ALEX commented this out, I don't think this code belongs here
#calculate regression weight of studies that were conducted by supporters of NRV model 
# DB.periph<-DB[DB$periph==T,]
# DB.periph$weight<-1/sqrt(DB.periph$g_var_calc^2)
# DB.periph$forNRV<-0
# DB.periph$forNRV[DB.periph$study_ID=="Polka1996"|DB.periph$study_ID=="Polka2011"]<-1
# forNRV.weight<-sum(DB.periph$weight[DB.periph$forNRV==1])/sum(DB.periph$weight)
# forNRV.weight

```

# Results

## Database description
We found a total of XX papers reporting XX (not mutually independent) effect sizes, see Table 1. XX effect sizes from XX papers have been submitted to or published in peer-reviewed journals (citations). The remaining xx effect sizes from xx papers are reported in preprints (cite), theses  (cite), conference presentations (proceedings, posters, or talks; cite).
Studies tended to have small sample sizes, with a median N of XX children (Range = XX, M = XX, Total: XX). A majority (N = XX) focused on North American English (NAE); for the other XX, children were learning FILL IN LANGUAGE. ADD OTHER DESCRIPTIVE INFORMATION ABOUT THE DEMOGRAPHICS OF THE SAMPLE, DISTRIBUTIONS IN TERMS OF N OF STUDIES OF EACH TYPE, ETC.
How many labs are represented?

## Publication bias
Evidence of bias at level of papers
Evidence of bias at level of literature

## Main effects
Heterogeneity
Moderators
age, type & interaction


# Discussion

- Age
- Type
- Interactions
- Another
- power
- heterogeneity
